---
layout: post
title: "경험적인 리스크 최소화하기"
author: 김태원
categories: MachineLearning
tags: [MachineLearning]
---

> *Shai Shalev-Shwartz와 Shai Ben-David의 [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf){:target="_blank"}을 읽으며 기록한 내용입니다.*
>
> <h3>관련 글</h3>
>
> [1. 학습이란 무엇인가?](https://pangmoo-ktw.github.io/pangmoo-KTW/uml0){:target="_blank"} 
>
> [2. 기계 학습은 언제 필요한가?](https://pangmoo-ktw.github.io/pangmoo-KTW/uml02){:target="_blank"}
>
> [3. 형식 모형: 통계적 프레임워크](https://pangmoo-ktw.github.io/pangmoo-KTW/uml21){:target="_blank"}

[형식 모형: 통계적 프레임워크](https://pangmoo-ktw.github.io/pangmoo-KTW/uml21)에서 말했듯이 학습 알고리즘은 입력으로 유한수열 $S$를 취한다.
이때 $S$는 미지의 확률 분포 $D$로 표본화되고 어떤 목적 함수 $f$로 라벨을 부여받은 것이다. 
그리고 학습 알고리즘은 예측기 $h_S:X\rightarrow Y$를 출력해야 한다.
학습 알고리즘의 목표는 미지의 $D$와 $f$에 대해 오류를 최소화하는 $h_s$를 찾는 것이다.

문제는 학습자가 $D$나 $f$도 모르기에 올바른 오류도 알 수 없다는 사실이다. 
이에 학습자가 계산할 수 있는 오류는 **훈련 오류**(training error)다.
훈련 오류란 $[m]=\lbrace1,\ldots,m\rbrace$와 아래 훈련 표본에 대해 분류기가 초래하는 오류다.

$$
L_s{h} := \frac{|\lbrace i\in[m] : h(x_i)\neq y_i\rbrace|}{m}.
$$

훈련 오류를 **경험적인 오류** 혹은 **경험적인 리스크**(emperical risk)라고 부르기도 한다. 

$L_s(h)$를 최소화하는 예측기 $h$에 따른 학습 패러다임은 **경험적 리스크 최소화**(Emperical Risk Minimization), 짧게 ERM이라고 한다.
