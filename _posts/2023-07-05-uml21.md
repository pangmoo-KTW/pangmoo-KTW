---
layout: post
title: "형식 모형: 통계적 프레임워크"
author: 김태원
categories: MachineLearning
tags: [MachineLearning]
---

> *Shai Shalev-Shwartz와 Shai Ben-David의 [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf){:target="_blank"}을 읽으며 기록한 내용입니다.*
>
> <h3>관련 글</h3>
>
> [1.1 학습이란 무엇인가?](https://pangmoo-ktw.github.io/pangmoo-KTW/uml0){:target="_blank"} 
>
> [1.2 기계 학습은 언제 필요한가?](https://pangmoo-ktw.github.io/pangmoo-KTW/uml02){:target="_blank"}

누가 조만간 마트에서 양파를 난생 처음 사본다고 하자.
경도나 색으로 양파의 질을 판단할 수 있지만, 그는 그런 기준을 모른다. 
그래서 좋은 양파에 관해 학습하고자 한다.
이런 문제에 관한 형식 모형(formal model)으로 **통계적 학습 프레임워크**(statistical learning framework)를 기술할 필요가 있겠다. 

우선 학습자에게 어떤 **입력**을 제공할지 확인하겠다.
학습자에게는 보통 **도메인 집합**(domain set), **라벨 집합**(lael set), **훈련 데이터**(training data)가 입력으로 필요하다.

도메인 집합이란 정의역과 비슷한 뜻을 지닌 집합 $X$로, 이 경우에는 모든 양파의 집합과 같은 것이겠다.
도메인 셋은 특징(feature)들의 벡터를 원소로 지닌다.
그리고 도메인상의 이들 벡터--점을 **인스턴스**(instances)라고 부르며 $X$를 인스턴스 공간(instance space)이라고 부른다. 

또한 이 경우 라벨 집합은 원소가 두 개인 집합으로 제한할 수 있다. 
맛있다는 감각을 나타내는 $1$과 맛없다는 감각을 나타내는 $0$에 대해 $Y$는 가능한 라벨 집합 $\lbrace0,0\rbrace, \ldots, \lbrace1,1\rbrace$을 전부 모은 집합이라고 하자. 

훈련 집합 $S=((x_1,y_1)\ldots(x_m,y_m))$는 데카르트곱 $X\times Y$상의 모든 순서쌍의 유한한 수열이다.
다시 말해 라벨이 부여된 인스턴스들의 수열이다. 
이들 인스턴스를 훈련 예제(training example)이라고 부른다.

도메인, 라벨, 훈련 집합으로 구성된 입력에 대해 학습자는 **예측 규칙**(prediction rule)을 출력해야 한다.
예측 규칙 $h:X\rightarrow Y$는 **가설**(hypothesis)이나 **분류기**(classifier)라고 부르기도 한다. 
이 경우, 학습자가 새로운 양파를 보면 그 질을 예측하여 라벨을 부여하는 규칙이 바로 함수 $h:X\rightarrow Y$다. 
학습 알고리즘 $A$가 훈련 수열 $S$로 반환하는 바를 가설 $A(S)$라고 표기하기도 한다.

그런데 훈련 데이터는 어떻게 생성하는가?
우선 우리는 인스턴스가 어떤 확률 분포 $D$에 의해 생성된다고 가정한다.
이때 핵심은 학습자가 $D$를 안다고 가정하지 않는다는 사실이다.
그리고 **옳은** 라벨 부여 함수 $f:X\rightarrow Y$가 있어서 $y_i=f(x_i)$라고 가정한다. 
학습자에게는 $f:X\rightarrow Y$가 주어지지 않는다.
$f:X\rightarrow Y$는 학습자가 알아내고자 하는 것이다. 

**분류기의 오류**, 가령 앞서 언급한 $h:X\rightarrow Y$의 오류는 $x\in X$에 대해 분포 $D$에 따라 $h(x)\neq f(x)$인 확률이다. 
주어진 도메인에 대한 부분집합 $A\subset X$와 확률 분포 $D$에 수 $D(A)$를 배정할 수도 있다.
$D(A)$는 $x\in A$라는 점을 관찰할 수 있는 정도를 결정한다. 
대부분 $A$는 하나의 사건을 지시하며 이 사건은 함수 $\pi:X\rightarrow\lbrace 0, 1 \rbrace$로 아래처럼 나타낸다.

$$
A=\lbrace x\in X:\pi(x)=1\rbrace.
$$

이에 분류기 혹은 예측 규칙 $h:X\rightarrow Y$의 오류를 다음처럼 정의한다.

$$
\begin{align*}
L_{(D,f)}(h) &:= D(A) \\
		&:= \underset{x~D}{\mathbb{P}}[h(x)\neq f(x)] \\
		&:= D(\lbrace x:h(x)\neq f(x)\rbrace).
\end{align*}
$$

즉 예측 규칙 $h:X\rightarrow Y$의 오류는 $h(x)\neq f(x)$인 $x$를 선택하는 확률이다. 
또한 여기서 $(D,f)$라는 첨자 표기는 오류가 확률 분포 $D$와 올바른 라벨 함수 $f$로 측정되었다는 것을 나타낸다. 
$L_{(D,f)}$를 **일반화 오류**, **리스크**라고 부르기도 한다.

핵심은 학습자에게는 양파의 확률 분포와 올바른 판단 기준--라벨 함수가 주어지지 않는다는 사실이다.
학습자는 우선 훈련 데이터, 가령 샘플 양파를 관찰해서 가설을 세운 다음 오류를 측정해야 한다.
