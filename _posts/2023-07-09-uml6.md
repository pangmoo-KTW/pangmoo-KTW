---
layout: post
title: "유한 가설류 (작성 중)"
author: 김태원
categories: MachineLearning
tags: [MachineLearning]
---

> *Shai Shalev-Shwartz와 Shai Ben-David의 [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf){:target="_blank"}을 읽으며 기록한 내용입니다.*
>
> <h3>관련 글</h3>
>
> [1. 학습이란 무엇인가?](https://pangmoo-ktw.github.io/pangmoo-KTW/uml0){:target="_blank"} 
>
> [2. 기계 학습은 언제 필요한가?](https://pangmoo-ktw.github.io/pangmoo-KTW/uml02){:target="_blank"}
>
> [3. 형식 모형: 통계적 프레임워크](https://pangmoo-ktw.github.io/pangmoo-KTW/uml21){:target="_blank"}
>
> [4. 경험적 리스크 최소화](https://pangmoo-ktw.github.io/pangmoo-KTW/uml22){:target="_blank"}
>
> [5. 귀납 편향에 따른 경험적 리스크 최소화](https://pangmoo-ktw.github.io/pangmoo-KTW/uml23){:target="_blank"}

[귀납 편향에 따른 경험적 리스크 최소화](https://pangmoo-ktw.github.io/pangmoo-KTW/uml23)에서 말했듯이 가설류(hypothesis class)를 제한하여 과적합을 방지할 수 있다.
그리고 **상계**는 가설류에 가할 수 있는 가장 간단한 제한이다.
다시 말해 예측기 $h\in\mathcal{H}$의 수를 고정하겠다.

이런 유한 가설류를 $\mathcal{H}$라고 부르겠다.
또 $\mathcal{H}$에 대해 경험적 리스크를 최소화하는 학습규칙은 아래처럼 표기하겠다.

$$
\textrm{ERM}_{\mathcal{H}}
$$

그리고 어떤 $f:X\rightarrow Y$로 라벨을 부여받은 훈련 예제 $S$가 있다고 하자. 
또 $S$에 $\textrm{ERM}_{\mathcal{H}}$를 적용한 결과를 $h_S$라고 하자.
정리하면 아래와 같다.

$$
h_S\in\underset{h\in\mathcal{H}}{\textrm{argmin }}L_S(h).
$$
