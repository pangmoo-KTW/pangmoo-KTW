---
layout: post
title: "유한 가설류 (작성 중)"
author: 김태원
categories: MachineLearning
tags: [MachineLearning]
---

> *Shai Shalev-Shwartz와 Shai Ben-David의 [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf){:target="_blank"}을 읽으며 기록한 내용입니다.*
>
> <h3>관련 글</h3>
>
> [1. 학습이란 무엇인가?](https://pangmoo-ktw.github.io/pangmoo-KTW/uml0){:target="_blank"} 
>
> [2. 기계 학습은 언제 필요한가?](https://pangmoo-ktw.github.io/pangmoo-KTW/uml02){:target="_blank"}
>
> [3. 형식 모형: 통계적 프레임워크](https://pangmoo-ktw.github.io/pangmoo-KTW/uml21){:target="_blank"}
>
> [4. 경험적 리스크 최소화](https://pangmoo-ktw.github.io/pangmoo-KTW/uml22){:target="_blank"}
>
> [5. 귀납 편향에 따른 경험적 리스크 최소화](https://pangmoo-ktw.github.io/pangmoo-KTW/uml23){:target="_blank"}

[귀납 편향에 따른 경험적 리스크 최소화](https://pangmoo-ktw.github.io/pangmoo-KTW/uml23)에서 말했듯이 가설류(hypothesis class)에 대한 제한으로 과적합을 방지할  수 있다.
클래스(class, 류)에 가할 수 있는 가장 간단한 제한은 바로 클래스의 크기에 **상계**를 두는 것이다.
다시 말해, 예측기 $h\in\mathcal{H}$의 수를 고정하는 것이다. 

이처럼 유한 가설류로 예측기를 제한하는 것은 적당한 해결책으로 보인다. 
가령 $\mathcal{H}$가 C++ 프로그램상 최대 $10^9$ 비트의 코드로 구현할 수 있는 모든 예측기의 집합이라고 하자. 
무한한 가설류에 대해 실수 표현을 부동소수점수 표현으로 이산화(discretize)하면, 가설류가 유한할 수 있을 것이다.

이제 유한류 $\mathcal{H}$에 대해 $\textrm{ERM}_{\mathcal{H}$ 학습 규칙의 성능을 분석하겠다.
어떤 $f:X\rightarrow Y$에 대해 레이블된 훈련 예제 $S$가 있고, $h_S$가 $\textrm{ERM}_H$을 $S$에 적용한 결과라고 하자.

$$
h_S\in\underset{h\in\mathcal{H}}{\textrm{argmin }}L_S(h).
$$

이에 아래 같은 단순화 가정을 취하겠다.

> **정의 2.1 (실현 가능성 가정)**
> 
> $h^*\in\mathcal{H}$가 존재하여 $L_{(D,f)}(h^*)=0$이라고 하자. 
> 이 가정은 $S$의 인스턴스가 $D$로 표본화되고 $f$로 라벨을 부여받아  임의의 표본에 대해 $1$의 확률을 지닌다면 $L_S(h^*)=0$일 것을 암시한다.
