---
layout: post
title: "확률론 개요"
author: 김태원
categories: MachineLearning
tags: [MachineLearning]
---

> - *José Unpingco의 [Python for Probability, Statistics, and Machine Learning](https://library.samdu.uz/files/7cbb6fdd660fb2c0f0580bfd6ed73040_Python%20for%20Probability,%20Statistics,%20and%20Machine%20Learning.pdf){:target="_blank"}을 읽으며 기록한 내용입니다.*
> - 관련 글 
>   - [NumPy개요](https://pangmoo-ktw.github.io/pangmoo-KTW/pythonML01)

어느 아이의 눈에 비친 세상 $f(x)$가 있다.
세상 자체는 $x$겠다. 
세상에는 여러 위험이 있다.
*어떤 위험이 아이에게 고통을 줄 것인가?*

부모는 이를 알 수 없다.
바로 이것이 **무작위적**이다.
어떤 위험이 실제로 아이에게 고통을 주면 불확실성은 증발한다. 
그 일이 일어나기 전까지 부모에게 주어진 것은 아이가 매해 나이를 먹으리라는 개념, 다시 말해 운명과 이 운명이 나타내는 도래할 위험의 가능성 뿐이다.

아래 같은 함수가 있다고 하자.

$$
f(x) = \begin{cases}0 < x \leq 1 \Rightarrow 1 \\
       1 < x \leq 2 \Rightarrow 2 \\
       x\leq 0\; \vee x>2 \Rightarrow 0\end{cases}_.
$$

이에 리만 적분을 사용한다.

$$
\int^2_0f(x)dx=1+2=3.
$$

즉 면적이 $1$과 $2$인 직사각형이 $f(x)$의 그래프로 존재한다.
르베그 적분은 리만 적분과 비슷하다.
다만 $x$ 좌표가 아니라 $y$ 좌표에 주목한다. 
이를테면 $f(x)=1$에 대해 어떤 $x$가 참인가?

이 경우에는 $x\in(0,1]$에 대해 참이다. 
그리하여 우리는 함수값과 이 함수값이 참인 $x$의 집합 간의 대응을 취한다.
이 경우는 $\lbrace(0,1]\rbrace$와 $\lbrace(1,2]\rbrace$다. 
적분을 계산하려면 함수값 $1,2$를 취해 이에 대응하는 구간 $\mu$의 크기를 재면 된다. 

$$
\int^2_0fd\mu=1\mu(\lbrace(0,1]\rbrace) + 2\mu(\lbrace(1,2]\rbrace).
$$

$\mu((0,1])=\mu((1,2])=1$이기에 리만 적분과 결과가 같다. 
다만 $\mu$ 함수의 도입으로 구간의 간격을 훨씬 덜 추잡스럽게 측정할 수 있다. 
일종의 자유다.
확률론은 이 자유에 근거한다.
그리고 확률론은 이 자유와 더불어 어떤 **독립**을 요구한다. 

두 자유변수 $X$와 $Y$가 독립인 필요충분조건은 아래와 같다.

$$
\mathbb{P}(X,Y)=\mathbb{P}(X)\mathbb{P}(Y).
$$

독립은 상관관계의 부재보다 강력한 개념이다. 
$X$와 $Y$가 집합 $\{1,2,3\}$에 균일하게 분포되었다고 하자.
$X$와 $Y$는 아래와 같다.

$$
X=\begin{cases}\omega=1 \Rightarrow 1 \\ \omega=2 \Rightarrow 0 \\\omega = 3 \Rightarrow = -1\end{cases}_,
Y=\begin{cases}\omega=1\Rightarrow 0 \\\omega = 2 \Rightarrow 1 \\\omega=3\Rightarrow = 0\end{cases}_.
$$

$\mathbb{E}(X)=0$이기에 $\mathbb{E}(X-\bar{X})\mathbb{E}(Y-\bar{Y})=0$이다.
따라서 상관관계가 없는 무작위 변수다. 
하지만 독립은 아니다.

$$
\mathbb{P}(X=1,Y=1)=0\neq\mathbb{P}(X=1)\mathbb{P}(Y=1)=\frac{1}{9}_.
$$

독립은 까다로워서 조건을 잘못 부여하면 깨진다. 
$Z$라는 조건에 따라 $X$와 $Y$가 독립이라는 것은 아래처럼 나타낸다.

$$
\mathbb{P}(X,Y,|Z)=\mathbb{P}(X|Z)\mathbb{P}(Y|Z).
$$

이에 두 독립 무작위 변수 $X_1,X_2\in\{0,1\}$가 있다고 하자.
$Z=X_1+X_2$를 정의한다. 
즉 $Z\in\{0,1,2\}$다. 
$Z=1$인 경우는 아래와 같다.

$$
\begin{aligned}
\mathbb{P}(X_1|Z=1)&>0\\
        \mathbb{P}(X_2|Z=1)&>0.
\end{aligned}
$$

$X_1,X_2$가 독립이더라도 $Z$에 의해 독립이 깨진다. 

$$
\mathbb{P}(X_1=1,X_2=1|Z=1)=0\neq\mathbb{P}(X_1=1|Z=1)\mathbb{P}(X_2=1|Z=1).
$$

이처럼 조건이 독립에 영향을 미친다는 사실은 확률론 모형의 핵심 주제다. 
예제 하나만 보겠다.
길이 $1$의 막대기를 세 조각으로 쪼갰다. 
이들 조각으로 삼각형을 만들 수 있는 확률은 얼마인가?

삼각형을 일종의 규약으로 다루겠다. 
우선 각 조각의 길이 $a,b,c$를 $Y>X$에 대해 아래처럼 둘 수 있다.

$$
\begin{aligned}
a &= X \\
     b &= Y-X \\
     c &= 1-Y.
\end{aligned}
$$

이때 $(a+b+c)/2$보다 $a,b,c$가 모두 작으면 세 조각은 삼각형을 이룬다. 
나머지는 파이썬에게 맡긴다.

```python
>>> import numpy as np
>>> x,y = np.random.rand(2,1000)
>>> a,b,c = x,(y-x),1-y 
>>> s = (a+b+c)/2 
>>> np.mean((s>a)&(s>b)&(s>c)&(y>x)) # 1/8 = 0.125a,b,c = x,(y-x),1-y
0.124 
```

$0.124\approx1/8$이다. 
