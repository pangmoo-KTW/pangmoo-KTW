---
layout: post
title: "확률론 개요"
author: 김태원
categories: MachineLearning
tags: [MachineLearning]
---

> - *José Unpingco의 [Python for Probability, Statistics, and Machine Learning](https://library.samdu.uz/files/7cbb6fdd660fb2c0f0580bfd6ed73040_Python%20for%20Probability,%20Statistics,%20and%20Machine%20Learning.pdf){:target="_blank"}을 읽으며 기록한 내용입니다.*
> - 관련 글 
>   - [NumPy개요](https://pangmoo-ktw.github.io/pangmoo-KTW/pythonML01)

어느 아이의 눈에 비친 세상 $f(x)$가 있다.
세상 자체는 $x$겠다. 
세상에는 여러 위험이 있다.
*어떤 위험이 아이에게 고통을 줄 것인가?*

부모는 이를 알 수 없다.
바로 이것이 **무작위적**이다.
어떤 위험이 실제로 아이에게 고통을 주면 불확실성은 증발한다. 
그 일이 일어나기 전까지 부모에게 주어진 것은 아이가 매해 나이를 먹으리라는 개념, 다시 말해 운명과 이 운명이 나타내는 도래할 위험의 가능성 뿐이다.

아래 같은 함수가 있다고 하자.

$$
f(x) = \begin{cases}0 < x \leq 1 \Rightarrow 1 \\
       1 < x \leq 2 \Rightarrow 2 \\
       x\leq 0\; \vee x>2 \Rightarrow 0\end{cases}_.
$$

이에 리만 적분을 사용한다.

$$
\int^2_0f(x)dx=1+2=3.
$$

즉 면적이 $1$과 $2$인 직사각형이 $f(x)$의 그래프로 존재한다.
이와 비슷한 르베그 적분은 다만 $x$ 좌표가 아니라 $y$ 좌표에 주목한다. 
이를테면 $f(x)=1$에 대해 어떤 $x$가 참인가?

이 경우에는 $x\in(0,1]$에 대해 참이다. 
그리하여 우리는 함수값과 이 함수값이 참인 $x$의 집합 간의 대응을 취한다.
이 경우는 $\lbrace(0,1]\rbrace$와 $\lbrace(1,2]\rbrace$다. 
적분을 계산하려면 함수값 $1,2$를 취해 이에 대응하는 구간 $\mu$의 크기를 재면 된다. 

$$
\int^2_0fd\mu=1\mu(\lbrace(0,1]\rbrace) + 2\mu(\lbrace(1,2]\rbrace).
$$

$\mu((0,1])=\mu((1,2])=1$이기에 리만 적분과 결과가 같다. 
다만 $\mu$ 함수의 도입으로 구간의 간격을 훨씬 덜 추잡스럽게 측정할 수 있다. 
이를 **측도**라고 부르기도 한다. 
그리고 측도는 확률론의 기초다.
왜 그럴까?

주사위 하나를 던지겠다. 
여섯 가지 결과가 있을 수 있다. 

$$
\Omega = \lbrace1,2,3,4,5,6\rbrace
$$

각 결과의 확률은 $1/6$이다. 
형식적으로 말하면 각 집합 $\lbrace1\rbrace,\lbrace2\rbrace,\ldots,\lbrace6\rbrace$의 측도는 아래와 같다. 

$$
\mu(\lbrace1\rbrace)=\mu(\lbrace2\rbrace)=\cdots=\mu\lbrace6\rbrace=\frac{1}{6}.
$$

이제 $\mu$ 함수에 다른 이름을 부여한다.
바로 **확률질량** 함수 $\mathbb{P}$다. 
$\mathbb{P}$는 하나의 집합을 실수 선상의 수 하나로 사상한다.

$$
\lbrace1\rbrace\longmapsto 1.
$$

확률질량 혹은 측도를 통해 주사위를 동전처럼 다룰 수도 있다.
주사위상의 수가 $3$ 이하라면 아랫면이고 $3$보다 크다면 윗면이라고 하겠다.
이에 두 집합이 나타난다.

$$
\lbrace 1,2,3\rbrace,\quad\lbrace4,5,6\rbrace.
$$

이들 집합이 겸치지 않는다는 점이 핵심이다. 
각 집합은 동일한 측도를 지닌다.

$$
\mathbb{P}(\lbrace1,2,3\rbrace)=\frac{1}{2}=\mathbb(\lbrace4,5,6\rbrace).
$$

여기까지는 실상 확률론이 필요 없다. 
다만 주사위를 두 번 던진다고 하자.
두 번의 주사위 던지기는 서로 **독립**이다.
다시 말해 두 번의 주사위 던지기는 서로 영향을 미치지 않는다.
가능한 결과의 집합은 아래와 같다.

$$
\Omega=\lbrace(1,1),(1,2),\ldots,(5,6),(6,6)\rbrace.
$$

이들 집합의 측도는 앞서 언급한 독립성 덕분에 개별 원소에 대한 두 측도의 곱과 같다.

$$
\mathbb{P}((1,2))=\mathbb{P}(\lbrace1\rbrace)\mathbb{P}(\lbrace2\rbrace)=\frac{1}{6^2}_.
$$

주사위를 두 번 던질 때 각 결과의 합이 $7$인 경우는 어떨까?
우선 측도 함수부터 둬야 한다.

$$
X:(a,b)\longmapsto (a+b).
$$

이어서 모든 $(a,b)$ 쌍을 그 합과 결합해야 한다.
파이썬으로 수행하면 아래와 같다. 

```python
>>> d = {(i,j):i+j for i in range(1,7) for j in range(1,7)}
>>> from collections import defaultdict
>>> dinv = defaultdict(list)
>>> for i,j in d.items():
...    dinv[j].append(i)
>>> dinv[7]
[(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)]
```

이제 각 요소(`item`)의 측도를 구해야 한다. 
독립을 가정했기에 `dinv`상의 개별 요소가 지니는 측도의 곱에 대한 합을 계산해야 한다.
또한 각 요소의 확률은 $1/36$이다.
따라서 위 `dinv`상의 순서쌍 개수인 $6$을 $36$으로 나누면 된다.

재밌는 점은 이 측도 함수가 `dinv`상의 순서쌍 개수 $2,3,\ldots,12$인 경우에 대한 확률을 자연스럽게 유도한다는 사실이다.

```python
>>> X = {i:len(j)/36. for i,j in dinv.items()}
>>> print(X)
{2: 0.027777777777777776, 3: 0.05555555555555555, 4: 0.08333333333333333, 5: 0.1111111111111111, 6: 0.1388888888888889, 7: 0.16666666666666666, 8: 0.1388888888888889, 9: 0.1111111111111111, 10: 0.08333333333333333, 11: 0.05555555555555555, 12: 0.027777777777777776}
```

그런데 이런 귀결은 두 번의 주사위 던지기가 독립적이기에 가능하다.
독립이란 무엇인가?
두 자유변수 $X$와 $Y$가 독립인 필요충분조건은 아래와 같다.

$$
\mathbb{P}(X,Y)=\mathbb{P}(X)\mathbb{P}(Y).
$$

독립은 까다로워서 조건을 잘못 부여하면 깨진다. 
$Z$라는 조건에 따라 $X$와 $Y$가 독립이라는 것은 아래처럼 나타낸다.

$$
\mathbb{P}(X,Y,|Z)=\mathbb{P}(X|Z)\mathbb{P}(Y|Z).
$$

이에 두 독립 무작위 변수 $X_1,X_2\in\{0,1\}$가 있다고 하자.
$Z=X_1+X_2$를 정의한다. 
즉 $Z\in\{0,1,2\}$다. 
$Z=1$인 경우는 아래와 같다.

$$
\begin{aligned}
\mathbb{P}(X_1|Z=1)&>0\\
        \mathbb{P}(X_2|Z=1)&>0.
\end{aligned}
$$

$X_1,X_2$가 독립이더라도 $Z$에 의해 독립이 깨진다. 

$$
\mathbb{P}(X_1=1,X_2=1|Z=1)=0\neq\mathbb{P}(X_1=1|Z=1)\mathbb{P}(X_2=1|Z=1).
$$

이처럼 조건이 독립에 영향을 미친다는 사실은 확률론 모형의 핵심 주제다. 
예제 하나만 보겠다.
길이 $1$의 막대기를 세 조각으로 쪼갰다. 
이들 조각으로 삼각형을 만들 수 있는 확률은 얼마인가?

삼각형을 일종의 규약으로 다루겠다. 
우선 각 조각의 길이 $a,b,c$를 $Y>X$에 대해 아래처럼 둘 수 있다.

$$
\begin{aligned}
a &= X \\
     b &= Y-X \\
     c &= 1-Y.
\end{aligned}
$$

이때 $(a+b+c)/2$보다 $a,b,c$가 모두 작으면 세 조각은 삼각형을 이룬다. 
나머지는 파이썬에게 맡긴다.

```python
>>> import numpy as np
>>> x,y = np.random.rand(2,1000)
>>> a,b,c = x,(y-x),1-y 
>>> s = (a+b+c)/2 
>>> np.mean((s>a)&(s>b)&(s>c)&(y>x)) # 1/8 = 0.125a,b,c = x,(y-x),1-y
0.124 
```

$0.124\approx1/8$이다. 
